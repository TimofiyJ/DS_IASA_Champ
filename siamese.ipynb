{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a77ae279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications, losses, optimizers, metrics, Model\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Flatten, Lambda, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import VGG16, vgg16\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85047076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/Tymof/OneDrive/Desktop/education/Practice/Hackaton/data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "160e7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "\n",
    "    images_train = np.array([])\n",
    "    labels_train = np.array([])\n",
    "    unique_train_label = np.array([])\n",
    "    map_train_label_indices = dict()\n",
    "\n",
    "    def __init__(self,data_src):\n",
    "        self.data_src = data_src\n",
    "        print(\"Loading the Dataset...\")\n",
    "        self.images_train, self.labels_train = self.preprocessing()\n",
    "        self.unique_train_label = np.unique(self.labels_train)\n",
    "        self.map_train_label_indices = {label: np.flatnonzero(self.labels_train == label) for label in\n",
    "                                        self.unique_train_label}\n",
    "        print('Preprocessing Done. Summary:')\n",
    "        print(\"Images train :\", self.images_train.shape)\n",
    "        print(\"Labels train :\", self.labels_train.shape)\n",
    "        print(\"Unique label :\", self.unique_train_label)\n",
    "\n",
    "    def normalize(self,x):\n",
    "        min_val = np.min(x)\n",
    "        max_val = np.max(x)\n",
    "        x = (x - min_val) / (max_val - min_val)\n",
    "        return x\n",
    "\n",
    "    def read_dataset(self):    \n",
    "        count = 0\n",
    "        for directory in os.listdir(self.data_src):\n",
    "            count += len([file for file in os.listdir(os.path.join(self.data_src, directory))])\n",
    "\n",
    "        X = [None] * count\n",
    "        y = [None] * count\n",
    "        idx = 0\n",
    "\n",
    "        for directory in os.listdir(self.data_src):\n",
    "            try:\n",
    "                print('Read directory: ', directory)\n",
    "                for pic in os.listdir(os.path.join(self.data_src, directory)):\n",
    "                    img = imread(os.path.join(self.data_src, directory, pic))\n",
    "                    \n",
    "                    has_channels = len(img.shape) == 3\n",
    "                    \n",
    "                    if has_channels:\n",
    "                        img = tf.image.rgb_to_grayscale(img)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                    img = tf.image.resize(img, (224,224))\n",
    "                    img = self.normalize(img)\n",
    "\n",
    "                    X[idx] = np.squeeze(np.asarray(img))\n",
    "                    y[idx] = directory\n",
    "                    idx += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Failed to read images from Directory: ', directory)\n",
    "                print('Exception Message: ', e)\n",
    "        print('Dataset loaded successfully.')\n",
    "        return X,y\n",
    "\n",
    "    def preprocessing(self):\n",
    "        X, y = self.read_dataset()\n",
    "\n",
    "        labels = list(set(y))\n",
    "        label_dict = dict(zip(labels, range(len(labels))))\n",
    "        Y = np.asarray([label_dict[label] for label in y])   \n",
    "   \n",
    "        \n",
    "        X_resized = np.zeros((len(X), 224, 224), dtype=np.float32)\n",
    "        for i, img in enumerate(X):\n",
    "            X_resized[i] = img \n",
    "\n",
    "        return X_resized, Y\n",
    "\n",
    "\n",
    "\n",
    "    def get_triplets(self):\n",
    "        label_l, label_r = np.random.choice(self.unique_train_label, 2, replace=False)\n",
    "        a, p = np.random.choice(self.map_train_label_indices[label_l],2, replace=False)\n",
    "        n = np.random.choice(self.map_train_label_indices[label_r])\n",
    "        return a, p, n\n",
    "\n",
    "    def get_triplets_batch(self):\n",
    "        idxs_a, idxs_p, idxs_n = [], [], []\n",
    "        n = len(self.labels_train)\n",
    "        for _ in range(n):\n",
    "            a, p, n = self.get_triplets()\n",
    "            idxs_a.append(a)\n",
    "            idxs_p.append(p)\n",
    "            idxs_n.append(n)\n",
    "            \n",
    "        anchor_dataset = tf.data.Dataset.from_tensor_slices(self.images_train[idxs_a,:])\n",
    "        positive_dataset = tf.data.Dataset.from_tensor_slices(self.images_train[idxs_p,:])\n",
    "        negative_dataset = tf.data.Dataset.from_tensor_slices(self.images_train[idxs_n, :])\n",
    "\n",
    "        dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4317373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Dataset...\n",
      "Read directory:  alert\n",
      "Read directory:  button\n",
      "Read directory:  card\n",
      "Read directory:  checkbox_checked\n",
      "Read directory:  checkbox_unchecked\n",
      "Read directory:  chip\n",
      "Read directory:  data_table\n",
      "Read directory:  dropdown_menu\n",
      "Read directory:  floating_action_button\n",
      "Read directory:  grid_list\n"
     ]
    }
   ],
   "source": [
    "dataset = PreProcessing(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15898e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_triplets_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(dataset.labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6be98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.take(round(count * 0.8))\n",
    "val_dataset = data.skip(round(count * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2619e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m         show(axs[i, \u001b[38;5;241m1\u001b[39m], positive[i])\n\u001b[0;32m     15\u001b[0m         show(axs[i, \u001b[38;5;241m2\u001b[39m], negative[i])\n\u001b[1;32m---> 18\u001b[0m visualize(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_numpy_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:633\u001b[0m, in \u001b[0;36mDatasetV2.as_numpy_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    626\u001b[0m       component_spec,\n\u001b[0;32m    627\u001b[0m       (tensor_spec\u001b[38;5;241m.\u001b[39mTensorSpec, ragged_tensor\u001b[38;5;241m.\u001b[39mRaggedTensorSpec,\n\u001b[0;32m    628\u001b[0m        sparse_tensor_lib\u001b[38;5;241m.\u001b[39mSparseTensorSpec, structure\u001b[38;5;241m.\u001b[39mNoneTensorSpec)):\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset.as_numpy_iterator()` is not supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets that produce values of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent_spec\u001b[38;5;241m.\u001b[39mvalue_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_NumpyIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4685\u001b[0m, in \u001b[0;36m_NumpyIterator.__init__\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m   4684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m-> 4685\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    706\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 710\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    746\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    747\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    748\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 749\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3450\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3451\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3452\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3454\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def visualize(anchor, positive, negative):\n",
    "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
    "\n",
    "    def show(ax, image):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "    axs = fig.subplots(3, 3)\n",
    "    for i in range(3):\n",
    "        show(axs[i, 0], anchor[i])\n",
    "        show(axs[i, 1], positive[i])\n",
    "        show(axs[i, 2], negative[i])\n",
    "\n",
    "\n",
    "visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "913e13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss:\n",
    "    \n",
    "    def embedding(self):\n",
    "        inp = Input(shape=(224,224,1))\n",
    "        \n",
    "        # first block\n",
    "        c1 = Conv2D(32, (7,7), activation='relu', padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(inp)\n",
    "        m1 = MaxPooling2D((2,2), padding='same')(c1)\n",
    "\n",
    "        # second block\n",
    "        c2 = Conv2D(64, (5,5), activation='relu', padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(m1)\n",
    "        m2 = MaxPooling2D((2,2), padding='same')(c2)\n",
    "\n",
    "        # third block\n",
    "        c3 = Conv2D(128, (3,3), activation='relu', padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(m2)\n",
    "        m3 = MaxPooling2D((2,2), padding='same')(c3)\n",
    "        \n",
    "        # fourth block\n",
    "        c4 = Conv2D(256, (2,2), activation='relu', padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(m3)\n",
    "        m4 = MaxPooling2D((2,2), padding='same')(c4)\n",
    "\n",
    "        c5 = Conv2D(28, (2,2), activation=None, padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(m4)\n",
    "        m5 = MaxPooling2D((2,2), padding='same')(c5)\n",
    "\n",
    "        f1 = Flatten()(m5)\n",
    "        \n",
    "        return Model(inputs=[inp], outputs=[f1], name='embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec004b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89ab9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # List metrics here so the `reset_states()` can be called automatically.\n",
    "        return [self.loss_tracker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d845388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TripletLoss()\n",
    "target_shape = (224,224)\n",
    "\n",
    "# Setup Network\n",
    "anchor_input = Input(name=\"anchor\", shape=target_shape + (1,))\n",
    "positive_input = Input(name=\"positive\", shape=target_shape + (1,))\n",
    "negative_input = Input(name=\"negative\", shape=target_shape + (1,))\n",
    "\n",
    "embedding = model.embedding()\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "034b2842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_ZipDataset element_spec=(TensorSpec(shape=(224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(224, 224), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "754e921a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Tymof\\AppData\\Local\\Temp\\ipykernel_21488\\3372527964.py\", line 23, in train_step\n        loss = self._compute_loss(data)\n    File \"C:\\Users\\Tymof\\AppData\\Local\\Temp\\ipykernel_21488\\3372527964.py\", line 45, in _compute_loss\n        ap_distance, an_distance = self.siamese_network(data)\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_1' (type Functional).\n    \n    Input 0 of layer \"embedding\" is incompatible with the layer: expected shape=(None, 224, 224, 1), found shape=(224, 224, 1, 1)\n    \n    Call arguments received by layer 'model_1' (type Functional):\n      • inputs=('tf.Tensor(shape=(224, 224, 1), dtype=float32)', 'tf.Tensor(shape=(224, 224, 1), dtype=float32)', 'tf.Tensor(shape=(224, 224, 1), dtype=float32)')\n      • training=None\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      3\u001b[0m  ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiamese_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      4\u001b[0m  EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[0;32m      5\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.001\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file69504ec5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m, in \u001b[0;36mSiameseModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 23\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Storing the gradients of the loss function with respect to the\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# weights/parameters.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msiamese_network\u001b[38;5;241m.\u001b[39mtrainable_weights)\n",
      "Cell \u001b[1;32mIn[32], line 45\u001b[0m, in \u001b[0;36mSiameseModel._compute_loss\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m---> 45\u001b[0m     ap_distance, an_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msiamese_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Computing the Triplet Loss by subtracting both distances and\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# making sure we don't get a negative value.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ap_distance \u001b[38;5;241m-\u001b[39m an_distance\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Tymof\\AppData\\Local\\Temp\\ipykernel_21488\\3372527964.py\", line 23, in train_step\n        loss = self._compute_loss(data)\n    File \"C:\\Users\\Tymof\\AppData\\Local\\Temp\\ipykernel_21488\\3372527964.py\", line 45, in _compute_loss\n        ap_distance, an_distance = self.siamese_network(data)\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_1' (type Functional).\n    \n    Input 0 of layer \"embedding\" is incompatible with the layer: expected shape=(None, 224, 224, 1), found shape=(224, 224, 1, 1)\n    \n    Call arguments received by layer 'model_1' (type Functional):\n      • inputs=('tf.Tensor(shape=(224, 224, 1), dtype=float32)', 'tf.Tensor(shape=(224, 224, 1), dtype=float32)', 'tf.Tensor(shape=(224, 224, 1), dtype=float32)')\n      • training=None\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5,min_delta=0.005, mode='max', verbose=1),\n",
    " ModelCheckpoint('siamese_weights.h5', monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=True),\n",
    " EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.001), metrics=['accuracy'])\n",
    "siamese_model.fit(train_dataset, epochs=1, validation_data=val_dataset,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "684a0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'siamese_weights'\n",
    "\n",
    "# Save the model\n",
    "siamese_model.save_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "776c58e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/train/button/0aacaf07-d9eb-4f37-ab0e-ebb456d06cce.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Tymof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:959\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_behavior:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims[key]\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "siamese_model.predict(\"./data/train/button/0aacaf07-d9eb-4f37-ab0e-ebb456d06cce.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
